# GeoBot-Final Project for HRI
INFO 5356-030: Introduction to Human-Robot Interaction

## Overview
This ROS2 package implements an interactive quiz robot that uses LiDAR sensing to detect participants and navigate to their positions. The system combines deliberative planning, reactive sensing, and coordinated execution in a three-layer control architecture.

## Package Structure

```
src/
â”œâ”€â”€ package.xml                      # ROS2 package manifest
â”œâ”€â”€ setup.py                         # Python package configuration
â”œâ”€â”€ robot_control_architecture_pkg/
â”‚   â”œâ”€â”€ robot_quiz_lidar_node.py    # Main control node (FSM + LiDAR)
â”‚   â””â”€â”€ simple_tts_node.py          # Text-to-speech node
â””â”€â”€ Other files                       # For compiling
```

## File Descriptions


### `robot_quiz_lidar_node.py` 
- **Subscribers**: `/scan` (LaserScan), `/quiz_correct` (String)
- **Publishers**: `/cmd_vel` (Twist), `/say_text` (String)
- **Finite State Machine**: 7-state cycle (IDLE â†’ ORIENT â†’ ADVANCE â†’ STOP_AT_GOAL â†’ TURN_BACK â†’ RETURN â†’ FACE_FORWARD)
- **Key Functions**:
  - `plan_for_answer()`: Calculates motion parameters based on LiDAR detection
  - `estimate_distance()`: Detects participants in target zones
  - `loop()`: Executes state transitions and publishes motion commands

### `simple_tts_node.py`
- Subscribes to `/say_text` topic
- Synthesizes speech using `pyttsx3` library
- Runs independently from main control loop

### `setup.py` & package.xml
- Package metadata (name, description, maintainer)
- Entry points for executables: `robot_quiz_lidar_node`, `simple_tts_node`
- Launch file configuration
- Defines package name, description, and maintainer information
- Declares dependencies: rclpy, sensor_msgs, geometry_msgs, std_msgs


## System Architecture

### Three-Layer Control Architecture

**Deliberative Layer (Top)**
- 7-state FSM defines sequential quiz interaction behavior
- States: IDLE â†’ ORIENT â†’ ADVANCE â†’ STOP_AT_GOAL â†’ TURN_BACK â†’ RETURN â†’ FACE_FORWARD â†’ IDLE

**Coordination Layer (Middle)**
- `plan_for_answer()`: Receives answers, queries reactive layer for distance, calculates timing parameters
- `loop()`: Generates velocity commands, triggers state transitions when `phase_end` is reached

**Reactive Layer (Bottom)**
- Subscribes to `/scan` for real-time LiDAR data
- `estimate_distance()`: Detects participants in 30Â° sectors around target directions (A/B/C/D)

### Data Flow
```
External Input (/quiz_correct) â”€â”€â”
                                  â”œâ”€â”€> Deliberative FSM â”€â”€> Coordination Layer â”€â”€> /cmd_vel (motion)
LiDAR (/scan) â”€â”€â”€> Reactive Layer â”˜                      â””â”€â”€> /say_text (speech)
```


## Demo Video
ðŸ“¹ [Watch the demo](https://drive.google.com/file/d/1EbMJ2bmjRJsywzv14wcY85f6ChZG1JWt/view?usp=sharing)

## Running the System

```bash
# Terminal 1: Launch robot control node
ros2 run quiz_robot robot_quiz_lidar_node

# Terminal 2: Launch TTS node
ros2 run quiz_robot simple_tts_node

# Terminal 3: Send quiz answers
ros2 topic pub /quiz_correct std_msgs/String "data: 'A'"
```


## Contributors
- Xinwei Xie: xx374@cornell.edu
- Rui Chen: rc986@cornell.edu
- Zifan Yang: zy489@cornell.edu
